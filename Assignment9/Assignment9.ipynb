{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pyOfL9FKDoG5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "USiZZJaUFQcT"
   },
   "outputs": [],
   "source": [
    "# Problem 1\n",
    "def load_data(filepath):\n",
    "  df = pd.read_csv(filepath)\n",
    "  df.head()\n",
    "  Y = df['5'].to_numpy()\n",
    "  del df['5']\n",
    "  X=df.to_numpy()\n",
    "  return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uVXIEJCvD0dJ"
   },
   "outputs": [],
   "source": [
    "X, y = load_data(\"mnist_train.csv\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\\\n",
    "                X, y, test_size=0.3, random_state=42)\n",
    "#One hot encoding of training labels \n",
    "Labels=pd.get_dummies(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8aYMsXGyDwqJ"
   },
   "outputs": [],
   "source": [
    "# Problem 2\n",
    "class Perceptron():\n",
    "    def __init__(self,x,y):\n",
    "        self.x = x/255\n",
    "        self.y = y\n",
    "        self.weights=[]\n",
    "        self.bias=[]\n",
    "        self.outputs=[]\n",
    "        self.derivatives=[]\n",
    "        self.activations=[]\n",
    "        \n",
    "    def connect(self,layer1,layer2):\n",
    "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "\n",
    "\n",
    "    def softmax(self,z):\n",
    "        e=np.exp(z)\n",
    "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
    "    \n",
    "    def max_log_likelihood(self,y_pred,y):\n",
    "        return y*np.log(y_pred)\n",
    "    \n",
    "    def delta_mll(self,y,y_pred):\n",
    "        return y_pred-y\n",
    "    \n",
    "    def forward_pass(self,x,y,weights,bias):\n",
    "        cost=0\n",
    "        self.outputs=[]\n",
    "        for i in range(len(weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            self.outputs.append(x) #append without adding ones array\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
    "            x=self.softmax(z)\n",
    "        self.outputs.append(x)\n",
    "        self.y_pred=x\n",
    "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
    "        cost=np.mean(np.sum(temp,axis=1))\n",
    "        return cost\n",
    "    \n",
    "    \n",
    "    def backward_pass(self,y,lr):\n",
    "        for i in range(len(self.weights)-1,-1,-1):\n",
    "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
    "            prev_term=self.delta_mll(y,self.y_pred)\n",
    "            self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
    "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "    \n",
    "    def train(self,batches,lr=1e-3,epoch=10):\n",
    "        for epochs in range(epoch):\n",
    "            samples=len(self.x)\n",
    "            c=0\n",
    "            for i in range(batches):\n",
    "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
    "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
    "              \n",
    "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
    "              self.backward_pass(y_batch,lr)\n",
    "            print(epochs,c/batches)\n",
    "    \n",
    "    def predict(self,x):\n",
    "        x=x/255\n",
    "        for i in range(len(self.weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
    "            x=self.softmax(z) \n",
    "        return np.argmax(x,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "colab_type": "code",
    "id": "TjgzI7Ss7dbD",
    "outputId": "fe2eff6f-534a-4c6a-ba87-f4a4e6f664c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0014700278799663021\n",
      "1 0.00180250229221284\n",
      "2 0.0017400307862695556\n",
      "3 0.0016732610383568358\n",
      "4 0.0015491532518861692\n",
      "5 0.001420696889167984\n",
      "6 0.0012221731225259716\n",
      "7 0.0010694470363131082\n",
      "8 0.0008693801005140858\n",
      "9 0.0007657911070430774\n",
      "10 0.0005900279618879865\n",
      "11 0.0004997371282559535\n",
      "12 0.0003934875170545859\n",
      "13 0.0003698314773426315\n",
      "14 0.00026310681098834195\n",
      "15 0.00020392537857128712\n",
      "16 0.0001496152051325453\n",
      "17 0.00012343250748374156\n",
      "18 9.795350020136143e-05\n",
      "19 7.898425213816967e-05\n",
      "20 6.865378826960783e-05\n",
      "21 6.402067008201463e-05\n",
      "22 6.351475261852474e-05\n",
      "23 6.436283101408294e-05\n",
      "24 6.473336401200048e-05\n",
      "25 6.518062877593675e-05\n",
      "26 6.543397967753736e-05\n",
      "27 6.575025985698043e-05\n",
      "28 6.598550681208671e-05\n",
      "29 6.567827252919952e-05\n"
     ]
    }
   ],
   "source": [
    "n=Perceptron(X_train,Labels)\n",
    "n.connect(X_train,Labels)\n",
    "n.train(batches=1000,lr=0.2,epoch=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "OhDzZkLr-Bg0",
    "outputId": "58dea9f6-b86b-4369-c100-1f91f5007ddd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([316, 330, 296, 352, 325, 282, 285, 328, 212, 274], dtype=int64),\n",
       " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288], dtype=int64))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=n.predict(X_test)\n",
    "np.bincount(n.predict(X_test)),np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "z2jK6jKC-Il3",
    "outputId": "be985372-bf87-4242-d9c2-5d2e78094c84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 88.53333333333333 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oYE-TondDhAH"
   },
   "outputs": [],
   "source": [
    "# Problem 3\n",
    "class Layer():\n",
    "    def __init__(self,size,activation='sigmoid'): \n",
    "        self.shape=(1,size)\n",
    "        self.activation=activation\n",
    "                \n",
    "class SingleLayerNeuralNetwork():\n",
    "    def __init__(self,x,y):\n",
    "        self.x=x/255  \n",
    "        self.y=y\n",
    "        self.weights=[]\n",
    "        self.bias=[]\n",
    "        self.outputs=[]\n",
    "        self.derivatives=[]\n",
    "        self.activations=[]\n",
    "        \n",
    "    def connect(self,layer1,layer2):\n",
    "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        if isinstance(layer2,Layer):\n",
    "            self.activations.append(layer2.activation)\n",
    "            \n",
    "    def activation(self,name,z,derivative=False):\n",
    "        if name=='sigmoid':\n",
    "            if derivative==False:\n",
    "                return 1/(1+np.exp(-z))\n",
    "            else:\n",
    "                return z*(1-z)\n",
    "        \n",
    "    def softmax(self,z):\n",
    "        e=np.exp(z)\n",
    "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
    "    \n",
    "    def max_log_likelihood(self,y_pred,y):\n",
    "        return y*np.log(y_pred)\n",
    "    \n",
    "    def delta_mll(self,y,y_pred):\n",
    "        #return y*(y_pred-1)\n",
    "        return y_pred-y\n",
    "    \n",
    "    def forward_pass(self,x,y,weights,bias):\n",
    "        cost=0\n",
    "        self.outputs=[]\n",
    "        for i in range(len(weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            self.outputs.append(x) #append without adding ones array\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
    "            if i==len(weights)-1:\n",
    "                x=self.softmax(z)\n",
    "            else:\n",
    "                x=self.activation(self.activations[i],z)\n",
    "        self.outputs.append(x)\n",
    "        self.y_pred=x\n",
    "        \n",
    "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
    "        cost=np.mean(np.sum(temp,axis=1))\n",
    "        return cost\n",
    "    \n",
    "    \n",
    "    def backward_pass(self,y,lr):\n",
    "        for i in range(len(self.weights)-1,-1,-1):\n",
    "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
    "            if i==len(self.weights)-1:\n",
    "                prev_term=self.delta_mll(y,self.y_pred)\n",
    "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
    "            else:\n",
    "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
    "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
    "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "                \n",
    "    \n",
    "    def train(self,batches,lr=1e-3,epoch=10):\n",
    "        for epochs in range(epoch):\n",
    "            samples=len(self.x)\n",
    "            c=0\n",
    "            for i in range(batches):\n",
    "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
    "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
    "              \n",
    "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
    "              self.backward_pass(y_batch,lr)\n",
    "            print(epochs,c/batches)\n",
    "    \n",
    "    def predict(self,x):\n",
    "        x=x/255\n",
    "        for i in range(len(self.weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
    "            if i==len(self.weights)-1:\n",
    "                x=self.softmax(z)\n",
    "            else:\n",
    "                x=self.activation(self.activations[i],z)\n",
    "\n",
    "        return np.argmax(x,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 885
    },
    "colab_type": "code",
    "id": "wffR59W-D9zR",
    "outputId": "8c31cd9c-7a48-4e1e-b839-4c37c4f88479"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0008345547260869195\n",
      "1 0.0005808408547376304\n",
      "2 0.0005881511282353178\n",
      "3 0.0005320890533170081\n",
      "4 0.0004375241218972611\n",
      "5 0.00030830095914011934\n",
      "6 0.00021786678438920657\n",
      "7 0.00016180586214947554\n",
      "8 0.00012419716561477788\n",
      "9 0.00010346951378854969\n",
      "10 9.054648343007574e-05\n",
      "11 8.070427461934618e-05\n",
      "12 7.237179922390795e-05\n",
      "13 6.493157639170142e-05\n",
      "14 5.823684822153643e-05\n",
      "15 5.23472159546952e-05\n",
      "16 4.7297866745867534e-05\n",
      "17 4.2973491572767175e-05\n",
      "18 3.929251366326887e-05\n",
      "19 3.620259862542618e-05\n",
      "20 3.364376328848572e-05\n",
      "21 3.1535570775793066e-05\n",
      "22 2.9777208704227535e-05\n",
      "23 2.8267481498885555e-05\n",
      "24 2.693167307998762e-05\n",
      "25 2.5724864284983916e-05\n",
      "26 2.4619665495430267e-05\n",
      "27 2.3597517563900294e-05\n",
      "28 2.264521303786158e-05\n",
      "29 2.1753364136882346e-05\n",
      "30 2.0915421092410794e-05\n",
      "31 2.012689251200012e-05\n",
      "32 1.938464938945524e-05\n",
      "33 1.868628952251103e-05\n",
      "34 1.8029625113792255e-05\n",
      "35 1.7412373358293814e-05\n",
      "36 1.6832064130291224e-05\n",
      "37 1.628609931363086e-05\n",
      "38 1.5771870652724946e-05\n",
      "39 1.528686794870847e-05\n",
      "40 1.48287495283737e-05\n",
      "41 1.4395374796012524e-05\n",
      "42 1.3984809715069398e-05\n",
      "43 1.3595316703967697e-05\n",
      "44 1.3225337309731554e-05\n",
      "45 1.287347268711322e-05\n",
      "46 1.253846450038948e-05\n",
      "47 1.2219177412008533e-05\n",
      "48 1.1914583545514707e-05\n",
      "49 1.1623748935951542e-05\n"
     ]
    }
   ],
   "source": [
    "n=SingleLayerNeuralNetwork(X_train,Labels)\n",
    "l1=Layer(100)\n",
    "n.connect(X_train,l1)\n",
    "n.connect(l1,Labels)\n",
    "n.train(batches=1000,lr=0.1,epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "dp0shsvSEWvN",
    "outputId": "5039d921-26a6-43ba-e8a4-eba8489e2e7f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([306, 326, 310, 327, 311, 258, 293, 330, 251, 288], dtype=int64),\n",
       " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288], dtype=int64))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=n.predict(X_test)\n",
    "np.bincount(n.predict(X_test)),np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "knkPcfW8EYx5",
    "outputId": "c319fd28-79f5-4a04-9aec-152f3acd2ad4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 92.06666666666666 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V_nmBd8mF4d5"
   },
   "outputs": [],
   "source": [
    "# Problem 4\n",
    "class Layer():\n",
    "    def __init__(self,size,activation='sigmoid'): \n",
    "        self.shape=(1,size)\n",
    "        self.activation=activation\n",
    "                \n",
    "class DoubleLayerNeuralNetwork():\n",
    "    def __init__(self,x,y):\n",
    "        self.x=x/255\n",
    "        self.y=y\n",
    "        self.weights=[]\n",
    "        self.bias=[]\n",
    "        self.outputs=[]\n",
    "        self.derivatives=[]\n",
    "        self.activations=[]\n",
    "        \n",
    "    def connect(self,layer1,layer2):\n",
    "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        if isinstance(layer2,Layer):\n",
    "            self.activations.append(layer2.activation)\n",
    "            \n",
    "    def activation(self,name,z,derivative=False):\n",
    "        if name=='sigmoid':\n",
    "            if derivative==False:\n",
    "                return 1/(1+np.exp(-z))\n",
    "            else:\n",
    "                return z*(1-z)\n",
    "        \n",
    "    def softmax(self,z):\n",
    "        e=np.exp(z)\n",
    "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
    "    \n",
    "    def max_log_likelihood(self,y_pred,y):\n",
    "        return y*np.log(y_pred)\n",
    "    \n",
    "    def delta_mll(self,y,y_pred):\n",
    "        return y_pred-y\n",
    "    \n",
    "    def forward_pass(self,x,y,weights,bias):\n",
    "        cost=0\n",
    "        self.outputs=[]\n",
    "        for i in range(len(weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            self.outputs.append(x)\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
    "            if i==len(weights)-1:\n",
    "                x=self.softmax(z)\n",
    "            else:\n",
    "                x=self.activation(self.activations[i],z)\n",
    "        self.outputs.append(x)\n",
    "        self.y_pred=x\n",
    "        \n",
    "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
    "        cost=np.mean(np.sum(temp,axis=1))\n",
    "        return cost\n",
    "    \n",
    "    \n",
    "    def backward_pass(self,y,lr):\n",
    "        for i in range(len(self.weights)-1,-1,-1):\n",
    "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
    "            if i==len(self.weights)-1:\n",
    "                prev_term=self.delta_mll(y,self.y_pred)  \n",
    "                # derivatives follow specific order,last three terms added new,rest from previous term  \n",
    "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
    "            else:\n",
    "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
    "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
    "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "                \n",
    "    \n",
    "    def train(self,batches,lr=1e-3,epoch=10):\n",
    "        for epochs in range(epoch):\n",
    "            samples=len(self.x)\n",
    "            c=0\n",
    "            for i in range(batches):\n",
    "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
    "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
    "              \n",
    "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
    "              self.backward_pass(y_batch,lr)\n",
    "            print(epochs,c/batches)\n",
    "    \n",
    "    def predict(self,x):\n",
    "        x=x/255\n",
    "        for i in range(len(self.weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
    "            if i==len(self.weights)-1:\n",
    "                x=self.softmax(z)\n",
    "            else:\n",
    "                x=self.activation(self.activations[i],z)\n",
    "        return np.argmax(x,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "xJY6j6AiGCI3",
    "outputId": "cde1989d-7336-40e9-b2ad-a6ec109102ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.000477297977996971\n",
      "1 0.0004370824213909472\n",
      "2 0.0003940384038046048\n",
      "3 0.00024447702463222226\n",
      "4 0.00014946380998389047\n",
      "5 9.527852840998494e-05\n",
      "6 5.9731629433823775e-05\n",
      "7 3.9970292447716846e-05\n",
      "8 2.9899724286116345e-05\n",
      "9 2.503206456253677e-05\n",
      "10 2.2875825693253987e-05\n",
      "11 2.2422580710767602e-05\n",
      "12 2.3058074722022042e-05\n",
      "13 2.4525473562403758e-05\n",
      "14 2.7133189644887257e-05\n",
      "15 3.068568946145714e-05\n",
      "16 3.41251153426844e-05\n",
      "17 3.64755606769258e-05\n",
      "18 3.743605152785791e-05\n",
      "19 3.698697041347594e-05\n"
     ]
    }
   ],
   "source": [
    "n=DoubleLayerNeuralNetwork(X_train,Labels)\n",
    "l1=Layer(100)\n",
    "l2=Layer(100)\n",
    "n.connect(X_train,l1)\n",
    "n.connect(l1,l2)\n",
    "n.connect(l2,Labels)\n",
    "n.train(batches=1000,lr=0.1,epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "mRAWf6C5GJgN",
    "outputId": "25d9b540-8076-4504-9130-dd1c5638ee66"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([311, 321, 321, 317, 322, 272, 283, 334, 251, 268], dtype=int64),\n",
       " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288], dtype=int64))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=n.predict(X_test)\n",
    "np.bincount(n.predict(X_test)),np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xmpGuKUbGMJ1",
    "outputId": "2ab4b7e5-9e2d-4eae-c400-cba966b2f011"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 91.36666666666666 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I-KrbHw9GlsV"
   },
   "outputs": [],
   "source": [
    "# Problem 5\n",
    "class Layer():\n",
    "    def __init__(self,size,activation='sigmoid'): \n",
    "        self.shape=(1,size)\n",
    "        self.activation=activation\n",
    "                \n",
    "class NeuralNetworkActivations():\n",
    "    def __init__(self,x,y):\n",
    "        self.x=x/255\n",
    "        self.y=y\n",
    "        self.weights=[]\n",
    "        self.bias=[]\n",
    "        self.outputs=[]\n",
    "        self.derivatives=[]\n",
    "        self.activations=[]\n",
    "        \n",
    "    def connect(self,layer1,layer2):\n",
    "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        if isinstance(layer2,Layer):\n",
    "            self.activations.append(layer2.activation)\n",
    "            \n",
    "    def activation(self,name,z,derivative=False):\n",
    "        if name=='sigmoid':\n",
    "            if derivative==False:\n",
    "                return 1/(1+np.exp(-z))\n",
    "            else:\n",
    "                return z*(1-z)\n",
    "        elif name=='relu':\n",
    "            if derivative==False:\n",
    "                return np.maximum(0.0,z)\n",
    "            else:\n",
    "              z[z<=0] = 0.0\n",
    "              z[z>0] = 1.0\n",
    "              return z\n",
    "        elif name=='tanh':\n",
    "          if derivative==False:\n",
    "                return np.tanh(z)\n",
    "          else:\n",
    "                return 1.0 - (np.tanh(z)) ** 2\n",
    "        \n",
    "    def softmax(self,z):\n",
    "        e=np.exp(z)\n",
    "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
    "    \n",
    "    def max_log_likelihood(self,y_pred,y):\n",
    "        return y*np.log(y_pred)\n",
    "    \n",
    "    def delta_mll(self,y,y_pred):\n",
    "        #return y*(y_pred-1)\n",
    "        return y_pred-y\n",
    "    \n",
    "    def forward_pass(self,x,y,weights,bias):\n",
    "        cost=0\n",
    "        self.outputs=[]\n",
    "        for i in range(len(weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            self.outputs.append(x) #append without adding ones array\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
    "            if i==len(weights)-1:\n",
    "                x=self.softmax(z)\n",
    "            else:\n",
    "                x=self.activation(self.activations[i],z)\n",
    "        self.outputs.append(x)\n",
    "        self.y_pred=x\n",
    "        \n",
    "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
    "        cost=np.mean(np.sum(temp,axis=1))\n",
    "        return cost\n",
    "    \n",
    "    \n",
    "    def backward_pass(self,y,lr):\n",
    "        for i in range(len(self.weights)-1,-1,-1):\n",
    "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
    "            if i==len(self.weights)-1:\n",
    "                prev_term=self.delta_mll(y,self.y_pred) \n",
    "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
    "            else:\n",
    "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
    "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
    "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "    \n",
    "    def train(self,batches,lr=1e-3,epoch=10):\n",
    "        for epochs in range(epoch):\n",
    "            samples=len(self.x)\n",
    "            c=0\n",
    "            for i in range(batches):\n",
    "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
    "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
    "              \n",
    "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
    "              self.backward_pass(y_batch,lr)\n",
    "            print(epochs,c/batches)\n",
    "    \n",
    "    def predict(self,x):\n",
    "        x=x/255\n",
    "        for i in range(len(self.weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
    "            if i==len(self.weights)-1:\n",
    "                x=self.softmax(z)\n",
    "            else:\n",
    "                x=self.activation(self.activations[i],z)\n",
    "        return np.argmax(x,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "JjlQBHZCHFWA",
    "outputId": "2be080cb-ee39-4697-b167-ed17e0f9e538"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.00019707411570678643\n",
      "1 0.00035331528653692856\n",
      "2 0.00040440511432289386\n",
      "3 0.00013959032431447746\n",
      "4 2.2669547202548738e-05\n",
      "5 2.9126372605376263e-05\n",
      "6 6.510555751085651e-06\n",
      "7 6.161987608993399e-05\n",
      "8 0.0003759319765426189\n",
      "9 1.926259689433873e-05\n",
      "10 3.844151890408797e-05\n",
      "11 0.00020800984795809666\n",
      "12 0.0004427368997836396\n",
      "13 0.0001716939545892781\n",
      "14 3.5342048136032495e-07\n",
      "15 9.442043577540581e-07\n",
      "16 1.5481396226043488e-05\n",
      "17 7.072208772977418e-07\n",
      "18 3.894621548570092e-07\n",
      "19 2.12127294264174e-06\n"
     ]
    }
   ],
   "source": [
    "n=NeuralNetworkActivations(X_train,Labels)\n",
    "l1=Layer(100,'sigmoid')\n",
    "l2=Layer(50, 'tanh')\n",
    "n.connect(X_train,l1)\n",
    "n.connect(l1,l2)\n",
    "n.connect(l2,Labels)\n",
    "n.train(batches=1000,lr=0.1,epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "qZfek1XaL2PR",
    "outputId": "ff1d0cea-00ad-407d-eaa0-8e11f8c4b5c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([303, 323, 285, 329, 328, 263, 308, 320, 250, 291], dtype=int64),\n",
       " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288], dtype=int64))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=n.predict(X_test)\n",
    "np.bincount(n.predict(X_test)),np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Sd2UmE1jNEUV",
    "outputId": "eb10fff3-62d2-4332-ca7b-0e9658d65120"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 91.23333333333333 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IyyRnpPaQRSS"
   },
   "outputs": [],
   "source": [
    "# Problem 6\n",
    "class Layer():\n",
    "    def __init__(self,size,activation='sigmoid'): \n",
    "        self.shape=(1,size)\n",
    "        self.activation=activation\n",
    "                \n",
    "class NeuralNetworkMomentum():\n",
    "    def __init__(self,x,y):\n",
    "        self.x=x/255 \n",
    "        self.y=y\n",
    "        self.weights=[]\n",
    "        self.bias=[]\n",
    "        self.outputs=[]\n",
    "        self.derivatives=[]\n",
    "        self.activations=[]\n",
    "        self.delta_weights=[]\n",
    "        self.delta_bias=[]\n",
    "        \n",
    "    def connect(self,layer1,layer2):\n",
    "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.delta_weights.append(np.zeros((layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.delta_bias.append(np.zeros((layer1.shape[1]+1,layer2.shape[1])))\n",
    "        if isinstance(layer2,Layer):\n",
    "            self.activations.append(layer2.activation)\n",
    "            \n",
    "    def activation(self,name,z,derivative=False):\n",
    "        if name=='sigmoid':\n",
    "            if derivative==False:\n",
    "                return 1/(1+np.exp(-z))\n",
    "            else:\n",
    "                return z*(1-z)\n",
    "        elif name=='relu':\n",
    "            if derivative==False:\n",
    "                return np.maximum(0.0,z)\n",
    "            else:\n",
    "              z[z<=0] = 0.0\n",
    "              z[z>0] = 1.0\n",
    "              return z\n",
    "        elif name=='tanh':\n",
    "          if derivative==False:\n",
    "                return np.tanh(z)\n",
    "          else:\n",
    "                return 1.0 - (np.tanh(z)) ** 2\n",
    "        \n",
    "    def softmax(self,z):\n",
    "        e=np.exp(z)\n",
    "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
    "    \n",
    "    def max_log_likelihood(self,y_pred,y):\n",
    "        return y*np.log(y_pred)\n",
    "    \n",
    "    def delta_mll(self,y,y_pred):\n",
    "        return y_pred-y\n",
    "    \n",
    "    def forward_pass(self,x,y,weights,bias):\n",
    "        cost=0\n",
    "        self.outputs=[]\n",
    "        for i in range(len(weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            self.outputs.append(x) #append without adding ones array\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
    "            if i==len(weights)-1:\n",
    "                x=self.softmax(z)\n",
    "            else:\n",
    "                x=self.activation(self.activations[i],z)\n",
    "        self.outputs.append(x)\n",
    "        self.y_pred=x\n",
    "        \n",
    "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
    "        cost=np.mean(np.sum(temp,axis=1))\n",
    "        return cost\n",
    "    \n",
    "    \n",
    "    def backward_pass(self,y,lr,momentum=False,beta=0.9):\n",
    "        for i in range(len(self.weights)-1,-1,-1):\n",
    "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
    "            if i==len(self.weights)-1:\n",
    "                prev_term=self.delta_mll(y,self.y_pred)  \n",
    "                # derivatives follow specific order,last three terms added new,rest from previous term  \n",
    "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
    "            else:\n",
    "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
    "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
    "            if momentum:\n",
    "                self.delta_weights[i]=beta*self.delta_weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "                self.delta_bias[i]=beta*self.delta_bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "                self.weights[i]=self.weights[i]+self.delta_weights[i]\n",
    "                self.bias[i]=self.bias[i]+self.delta_bias[i]\n",
    "            else:\n",
    "                self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "                self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "    \n",
    "    def train(self, batches,lr=1e-3,epoch=10,beta=0.5):\n",
    "        for epochs in range(epoch):\n",
    "            samples=len(self.x)\n",
    "            c=0\n",
    "            for i in range(batches):\n",
    "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
    "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
    "              \n",
    "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
    "              self.backward_pass(y_batch,lr,momentum=True,beta=0.5)\n",
    "            print(epochs,c/batches)\n",
    "    \n",
    "    def predict(self,x):\n",
    "        x=x/255\n",
    "        for i in range(len(self.weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
    "            if i==len(self.weights)-1:\n",
    "                x=self.softmax(z)\n",
    "            else:\n",
    "                x=self.activation(self.activations[i],z)\n",
    "        return np.argmax(x,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "5KieS6HyQuwx",
    "outputId": "72ae514a-6211-4948-db17-cd5092f16b4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.002727644517620976\n",
      "1 0.0008321167885062017\n",
      "2 0.0005295581276711337\n",
      "3 0.0005073442910288121\n",
      "4 0.00017014822092340065\n",
      "5 0.00041420622740913457\n",
      "6 0.0004260597441019876\n",
      "7 0.00011557345355528839\n",
      "8 0.00012277878254181152\n",
      "9 0.00032112889312066516\n",
      "10 3.0487217525717035e-06\n",
      "11 0.00010600308699715057\n",
      "12 7.29697299886957e-06\n",
      "13 5.307886845787543e-05\n",
      "14 2.874540141977016e-06\n",
      "15 3.2569898152850494e-06\n",
      "16 6.97221991823842e-07\n",
      "17 7.285888753687372e-07\n",
      "18 6.787183644664866e-07\n",
      "19 6.327255765025357e-07\n"
     ]
    }
   ],
   "source": [
    "n=NeuralNetworkMomentum(X_train,Labels)\n",
    "l1=Layer(100,'sigmoid')\n",
    "l2=Layer(50, 'tanh')\n",
    "n.connect(X_train,l1)\n",
    "n.connect(l1,l2)\n",
    "n.connect(l2,Labels)\n",
    "n.train(batches=500,lr=0.1,epoch=20,beta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "M6I2UDHGQy7c",
    "outputId": "32569bcd-dc09-4d76-8c74-adb0949961cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([304, 329, 291, 322, 315, 271, 289, 352, 247, 280], dtype=int64),\n",
       " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288], dtype=int64))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=n.predict(X_test)\n",
    "np.bincount(n.predict(X_test)),np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "apHXOBwRQ60d",
    "outputId": "bbad07c7-96a6-4ad9-dda1-627f6cf30b32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 93.03333333333333 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ML_Assignment9.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
